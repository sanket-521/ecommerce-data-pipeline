# ðŸ§± End-to-End Data Engineering Project: E-Commerce Sales Data Pipeline

## ðŸ“– Overview
This project demonstrates an **end-to-end data engineering pipeline** using **AWS Glue (PySpark)**, **Databricks**, and **Power BI**.  
It processes e-commerce sales data from raw files in S3 to analytics-ready fact and dimension tables using a **Star Schema**.

---

## ðŸ§© Architecture
![Architecture](screenshots/architecture_diagram.png)

---

## âš™ï¸ Tech Stack
- **AWS S3** â€“ Data Lake Storage  
- **AWS Glue (PySpark)** â€“ ETL & Data Cleaning  
- **Databricks** â€“ Data Modeling & Analytics  
- **Power BI** â€“ Visualization  
- **Python, SQL, PySpark** â€“ Programming  

---

## ðŸ“Š Data Pipeline Flow

| Stage | Description | Output |
|--------|--------------|--------|
| Raw | Raw CSV from source | `s3://.../raw/` |
| Staging | Cleaned with PySpark (Glue) | `s3://.../staging/` |
| Curated | Incrementally updated | `s3://.../curated/` |
| Warehouse | Star schema: Fact & Dimensions | `s3://.../warehouse/` |
| Analytics | Aggregated KPIs | `s3://.../analytics/` |

---

## ðŸ§± Data Model (Star Schema)

**Fact Table:** `fact_sales`  
**Dimension Tables:** `dim_customer`, `dim_product`, `dim_region`, `dim_date`

![Schema](docs/schema_design.png)

---

## ðŸ” Incremental ETL Logic
```python
max_date = existing_df.select(spark_max("Order_Date")).collect()[0][0]
df_incremental = df_new.filter(col("Order_Date") > max_date)
df_incremental.write.mode("append").parquet(curated_path)
